{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import threading\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf, split, window, from_unixtime, avg,  when, lit\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, LongType, TimestampType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "PREFIX = \"10m\"\n",
    "symbols_features = {\n",
    "    \"BP\": [\"volume\", \"volatility\", \"market_sentiment\", \"trading_activity\", \"price\"],\n",
    "    \"COP\": [\"volume\", \"volatility\", \"market_sentiment\", \"trading_activity\", \"price\"],\n",
    "    \"SHEL\": [\"volume\", \"volatility\", \"market_sentiment\", \"trading_activity\", \"price\"],\n",
    "    \"XOM\": [\"volume\", \"volatility\", \"market_sentiment\", \"trading_activity\", \"price\"],\n",
    "    \"ETHEREUM\": [\"bid\", \"ask\", \"spread_raw\", \"spread_table\", \"price\"],\n",
    "}\n",
    "\n",
    "if len(sys.argv) != 2:\n",
    "    raise ValueError(\"Please provide the TARGET_SYMBOL as a command-line argument.\")\n",
    "\n",
    "TARGET_SYMBOL = \"BP\"\n",
    "\n",
    "if TARGET_SYMBOL not in symbols_features:\n",
    "    raise ValueError(f\"TARGET_SYMBOL '{TARGET_SYMBOL}' is not defined in symbols_features.\")\n",
    "\n",
    "FEATURE_COLUMNS = symbols_features[TARGET_SYMBOL]\n",
    "NUM_FEATURES = len(FEATURE_COLUMNS)\n",
    "\n",
    "# Model and Checkpoint Paths with PREFIX\n",
    "MODEL_BASE_PATH = f\"./{PREFIX}/models/{TARGET_SYMBOL}\"\n",
    "CHECKPOINT_PATH_AGG = f\"./{PREFIX}/checkpoint/dir/{TARGET_SYMBOL}_agg\"\n",
    "CHECKPOINT_PATH_PRED = f\"./{PREFIX}/checkpoint/dir/{TARGET_SYMBOL}_pred\"\n",
    "\n",
    "os.makedirs(MODEL_BASE_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH_AGG, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH_PRED, exist_ok=True)\n",
    "\n",
    "CASSANDRA_KEYSPACE = \"stream_predictions\"\n",
    "CASSANDRA_TABLE = \"model_predictions_10m\"\n",
    "\n",
    "KAFKA_BROKERS = \"localhost:9092\"\n",
    "KAFKA_TOPIC = \"model-topic\"\n",
    "\n",
    "MAX_ITER = 50\n",
    "REG_PARAM = 0.01\n",
    "ELASTIC_NET_PARAM = 0.5\n",
    "\n",
    "HISTORICAL_MODEL_BASE_PATH = f\"./{PREFIX}/historical_models/{TARGET_SYMBOL}\"\n",
    "os.makedirs(HISTORICAL_MODEL_BASE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_port_offset = {\n",
    "    \"ETHEREUM\": 0,\n",
    "    \"SHEL\": 1,\n",
    "    \"BP\": 2,\n",
    "    \"COP\": 3,\n",
    "    \"XOM\": 4\n",
    "}\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f\"ContinuousTrainingLinearRegression_{TARGET_SYMBOL}\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "            \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\") \\\n",
    "    .config(\"spark.ui.port\", str(4050 + spark_port_offset[TARGET_SYMBOL])) \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    latest_model_path = os.path.join(MODEL_BASE_PATH, f\"latest_model.txt\")\n",
    "    if os.path.exists(latest_model_path):\n",
    "        try:\n",
    "            with open(latest_model_path, \"r\") as f:\n",
    "                model_path = f.read().strip()\n",
    "            model = LinearRegressionModel.load(model_path)\n",
    "            print(f\"[{TARGET_SYMBOL}] Loaded model from {model_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"[{TARGET_SYMBOL}] Error loading model: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_model(model):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = os.path.join(MODEL_BASE_PATH, f\"model_{timestamp}\")\n",
    "    model.write().overwrite().save(model_path)\n",
    "\n",
    "    latest_model_path = os.path.join(MODEL_BASE_PATH, \"latest_model.txt\")\n",
    "    with open(latest_model_path, \"w\") as f:\n",
    "        f.write(model_path)\n",
    "\n",
    "def load_historical_model():\n",
    "    \"\"\"\n",
    "    Load the historical model from disk\n",
    "    \"\"\"\n",
    "    latest_model_path = os.path.join(HISTORICAL_MODEL_BASE_PATH, f\"latest_model.txt\")\n",
    "    if os.path.exists(latest_model_path):\n",
    "        try:\n",
    "            with open(latest_model_path, \"r\") as f:\n",
    "                model_path = f.read().strip()\n",
    "            model = LinearRegressionModel.load(model_path)\n",
    "            print(f\"[{TARGET_SYMBOL}] Loaded historical model from {model_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"[{TARGET_SYMBOL}] Error loading historical model: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_historical_model(model):\n",
    "    \"\"\"\n",
    "    Save the historical model to disk\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = os.path.join(HISTORICAL_MODEL_BASE_PATH, f\"model_{timestamp}\")\n",
    "    model.write().overwrite().save(model_path)\n",
    "\n",
    "    latest_model_path = os.path.join(HISTORICAL_MODEL_BASE_PATH, \"latest_model.txt\")\n",
    "    with open(latest_model_path, \"w\") as f:\n",
    "        f.write(model_path)\n",
    "    print(f\"[{TARGET_SYMBOL}] Saved historical model to {model_path}\")\n",
    "\n",
    "def update_model(batch_df: DataFrame, batch_id: int):\n",
    "    global lr_model\n",
    "\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        print(f\"[{TARGET_SYMBOL}] Batch {batch_id} is empty. Skipping update.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        lr = LinearRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            maxIter=MAX_ITER,\n",
    "            regParam=REG_PARAM,\n",
    "            elasticNetParam=ELASTIC_NET_PARAM\n",
    "        )\n",
    "        new_model = lr.fit(batch_df)\n",
    "        lr_model = new_model\n",
    "\n",
    "        print(f\"[{TARGET_SYMBOL}] Updated model with batch {batch_id}\")\n",
    "        print(f\"[{TARGET_SYMBOL}] Coefficients: {lr_model.coefficients}\")\n",
    "        print(f\"[{TARGET_SYMBOL}] Intercept: {lr_model.intercept}\")\n",
    "\n",
    "        save_model(lr_model)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{TARGET_SYMBOL}] Error in batch {batch_id}: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from typing import Tuple, Dict\n",
    "SELECTED_MODEL_TYPE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_historical_models(historical_features, test_size: float = 0.2) -> Tuple[object, str, Dict]:\n",
    "    \"\"\"\n",
    "    Trains multiple models on historical data and evaluates their performance.\n",
    "    Returns the best model, its type, and evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Split data into training and test sets\n",
    "    train_data, test_data = historical_features.randomSplit([1 - test_size, test_size], seed=42)\n",
    "    \n",
    "    # Initialize models with their parameters\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            maxIter=MAX_ITER,\n",
    "            regParam=REG_PARAM,\n",
    "            elasticNetParam=ELASTIC_NET_PARAM\n",
    "        ),\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            numTrees=100,\n",
    "            maxDepth=10,\n",
    "            seed=42\n",
    "        ),\n",
    "        'GradientBoosting': GBTRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            maxIter=100,\n",
    "            maxDepth=5\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    model_metrics = {}\n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    best_model_type = None\n",
    "    \n",
    "    print(f\"[{TARGET_SYMBOL}] Training and evaluating models...\")\n",
    "    \n",
    "    for model_type, model in models.items():\n",
    "        try:\n",
    "            # Train model\n",
    "            print(f\"[{TARGET_SYMBOL}] Training {model_type}...\")\n",
    "            trained_model = model.fit(train_data)\n",
    "            \n",
    "            # Make predictions on test data\n",
    "            predictions = trained_model.transform(test_data)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            evaluator.setMetricName(\"mae\")\n",
    "            mae = evaluator.evaluate(predictions)\n",
    "            evaluator.setMetricName(\"r2\")\n",
    "            r2 = evaluator.evaluate(predictions)\n",
    "            \n",
    "            model_metrics[model_type] = {\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'r2': r2\n",
    "            }\n",
    "            \n",
    "            print(f\"[{TARGET_SYMBOL}] {model_type} metrics:\")\n",
    "            print(f\"    RMSE: {rmse:.4f}\")\n",
    "            print(f\"    MAE: {mae:.4f}\")\n",
    "            print(f\"    R2: {r2:.4f}\")\n",
    "            \n",
    "            # Update best model if current model performs better\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_model = trained_model\n",
    "                best_model_type = model_type\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[{TARGET_SYMBOL}] Error training {model_type}: {str(e)}\")\n",
    "            model_metrics[model_type] = None\n",
    "    \n",
    "    if best_model is None:\n",
    "        raise ValueError(\"No models were successfully trained\")\n",
    "    \n",
    "    # Update global selected model type\n",
    "    global SELECTED_MODEL_TYPE\n",
    "    SELECTED_MODEL_TYPE = best_model_type\n",
    "    \n",
    "    # Save the best model\n",
    "    save_historical_model(best_model)\n",
    "    \n",
    "    print(f\"[{TARGET_SYMBOL}] Best performing model: {best_model_type}\")\n",
    "    return best_model, best_model_type, model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_historical_data():\n",
    "    \"\"\"\n",
    "    Loads historical data from Cassandra and trains initial model with debug info\n",
    "    \"\"\"\n",
    "    print(f\"[{TARGET_SYMBOL}] Loading historical data from Cassandra...\")\n",
    "    \n",
    "    # Load historical data from Cassandra\n",
    "    historical_df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(table=CASSANDRA_TABLE, keyspace=CASSANDRA_KEYSPACE) \\\n",
    "        .load() \\\n",
    "        .filter(col(\"symbol\") == TARGET_SYMBOL)\n",
    "\n",
    "    # Debug print\n",
    "    print(f\"[{TARGET_SYMBOL}] Initial schema:\")\n",
    "    historical_df.printSchema()\n",
    "    print(f\"[{TARGET_SYMBOL}] Initial count: {historical_df.count()}\")\n",
    "\n",
    "    if historical_df.rdd.isEmpty():\n",
    "        raise ValueError(f\"No historical data found for symbol {TARGET_SYMBOL}\")\n",
    "\n",
    "    # Parse input_data JSON to get features\n",
    "    for feature in FEATURE_COLUMNS:\n",
    "        historical_df = historical_df.withColumn(\n",
    "            feature,\n",
    "            F.get_json_object(col(\"input_data\"), f\"$.{feature}\").cast(\"double\")\n",
    "        )\n",
    "    \n",
    "    # Debug print after feature extraction\n",
    "    print(f\"[{TARGET_SYMBOL}] Schema after feature extraction:\")\n",
    "    historical_df.printSchema()\n",
    "    print(f\"[{TARGET_SYMBOL}] Sample data after feature extraction:\")\n",
    "    historical_df.select(\"symbol\", *FEATURE_COLUMNS, \"label\").show(5)\n",
    "\n",
    "    # Add event_time column if not exists\n",
    "    if \"event_time\" not in historical_df.columns:\n",
    "        historical_df = historical_df.withColumn(\n",
    "            \"event_time\",\n",
    "            (col(\"timestamp\") / 1000).cast(TimestampType())\n",
    "        )\n",
    "\n",
    "    # Group by 10-minute windows\n",
    "    historical_windowed = historical_df \\\n",
    "        .groupBy(\n",
    "            window(col(\"event_time\"), \"10 minutes\"),\n",
    "            col(\"symbol\")\n",
    "        ) \\\n",
    "        .agg(\n",
    "            *[\n",
    "                F.avg(col(feature)).alias(f\"avg_{feature}\")\n",
    "                for feature in FEATURE_COLUMNS\n",
    "            ],\n",
    "            F.avg(col(\"label\")).alias(\"label\")\n",
    "        )\n",
    "\n",
    "    # Debug print after windowing\n",
    "    print(f\"[{TARGET_SYMBOL}] Schema after windowing:\")\n",
    "    historical_windowed.printSchema()\n",
    "    print(f\"[{TARGET_SYMBOL}] Sample data after windowing:\")\n",
    "    historical_windowed.show(5)\n",
    "\n",
    "    # Check for null values\n",
    "    for feature in [f\"avg_{feature}\" for feature in FEATURE_COLUMNS] + [\"label\"]:\n",
    "        null_count = historical_windowed.filter(col(feature).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            print(f\"[{TARGET_SYMBOL}] WARNING: Found {null_count} null values in {feature}\")\n",
    "\n",
    "    # Prepare features\n",
    "    assembler_historical = VectorAssembler(\n",
    "        inputCols=[f\"avg_{feature}\" for feature in FEATURE_COLUMNS],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        historical_features = assembler_historical.transform(historical_windowed)\n",
    "        \n",
    "        # Debug print assembled features\n",
    "        print(f\"[{TARGET_SYMBOL}] Schema after feature assembly:\")\n",
    "        historical_features.printSchema()\n",
    "        print(f\"[{TARGET_SYMBOL}] Sample data after feature assembly:\")\n",
    "        historical_features.select(\"features\", \"label\").show(5, truncate=False)\n",
    "        \n",
    "        # Check for problematic data\n",
    "        invalid_count = historical_features.filter(\n",
    "            col(\"features\").isNull() | \n",
    "            col(\"label\").isNull() |\n",
    "            F.isnan(\"label\")\n",
    "        ).count()\n",
    "        \n",
    "        if invalid_count > 0:\n",
    "            print(f\"[{TARGET_SYMBOL}] WARNING: Found {invalid_count} rows with null/NaN features or labels\")\n",
    "            \n",
    "            # Remove problematic rows\n",
    "            historical_features = historical_features.filter(\n",
    "                col(\"features\").isNotNull() & \n",
    "                col(\"label\").isNotNull() &\n",
    "                ~F.isnan(\"label\")\n",
    "            )\n",
    "            print(f\"[{TARGET_SYMBOL}] Cleaned data count: {historical_features.count()}\")\n",
    "            \n",
    "        best_model, model_type, metrics = train_and_evaluate_historical_models(historical_features)\n",
    "        \n",
    "        print(f\"[{TARGET_SYMBOL}] Selected model type: {SELECTED_MODEL_TYPE}\")\n",
    "        print(f\"[{TARGET_SYMBOL}] Model evaluation metrics:\")\n",
    "        for model_type, model_metrics in metrics.items():\n",
    "            if model_metrics:\n",
    "                print(f\"\\n{model_type}:\")\n",
    "                for metric, value in model_metrics.items():\n",
    "                    print(f\"    {metric}: {value:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[{TARGET_SYMBOL}] Error during model training and evaluation: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1726:====>        (68 + 2) / 200][Stage 1728:>               (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Start label updater stream\n",
    "def update_labels(batch_df: DataFrame, batch_id: int):\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Calculate 10-minute average prices and select window bounds\n",
    "    avg_prices = batch_df.groupBy(\n",
    "        window(col(\"event_time\"), \"10 minutes\")\n",
    "    ).agg(\n",
    "        avg(\"label\").alias(\"actual_price\")\n",
    "    ).select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"actual_price\")\n",
    "    )\n",
    "\n",
    "    # For each window, update records in Cassandra\n",
    "    for row in avg_prices.collect():\n",
    "        window_start = row['window_start']\n",
    "        window_end = row['window_end']\n",
    "        actual_price = row['actual_price']\n",
    "        \n",
    "        # Read matching records\n",
    "        matching_records = spark.read \\\n",
    "            .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "            .options(table=CASSANDRA_TABLE, keyspace=CASSANDRA_KEYSPACE) \\\n",
    "            .load() \\\n",
    "            .filter(\n",
    "                (col(\"symbol\") == TARGET_SYMBOL) &\n",
    "                (col(\"event_time\") >= window_start) &\n",
    "                (col(\"event_time\") < window_end)\n",
    "            )\n",
    "\n",
    "        # Update records with actual price\n",
    "        if not matching_records.rdd.isEmpty():\n",
    "            matching_records \\\n",
    "                .withColumn(\"label\", F.lit(actual_price)) \\\n",
    "                .write \\\n",
    "                .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .options(table=CASSANDRA_TABLE, keyspace=CASSANDRA_KEYSPACE) \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] Loading historical data from Cassandra...\n",
      "[BP] Initial schema:\n",
      "root\n",
      " |-- symbol: string (nullable = false)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- input_data: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      " |-- prediction_historical: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] Initial count: 214764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] Schema after feature extraction:\n",
      "root\n",
      " |-- symbol: string (nullable = false)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- input_data: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      " |-- prediction_historical: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- volatility: double (nullable = true)\n",
      " |-- market_sentiment: double (nullable = true)\n",
      " |-- trading_activity: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "[BP] Sample data after feature extraction:\n",
      "+------+---------+----------+----------------+----------------+-----+------------------+\n",
      "|symbol|   volume|volatility|market_sentiment|trading_activity|price|             label|\n",
      "+------+---------+----------+----------------+----------------+-----+------------------+\n",
      "|    BP|4369160.0|     1.012|           0.998|           88.95|31.09|31.113341463414663|\n",
      "|    BP|4369160.0|     1.012|           0.998|           88.95|31.09|31.113341463414663|\n",
      "|    BP|4369160.0|     1.012|           0.998|           88.95|31.09|31.113341463414663|\n",
      "|    BP|4369160.0|     1.012|           0.998|           88.95|31.08|31.113341463414663|\n",
      "|    BP|4296939.0|     1.029|           0.962|            86.1| 31.2|31.113341463414663|\n",
      "+------+---------+----------+----------------+----------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[BP] Schema after windowing:\n",
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- symbol: string (nullable = false)\n",
      " |-- avg_volume: double (nullable = true)\n",
      " |-- avg_volatility: double (nullable = true)\n",
      " |-- avg_market_sentiment: double (nullable = true)\n",
      " |-- avg_trading_activity: double (nullable = true)\n",
      " |-- avg_price: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n",
      "[BP] Sample data after windowing:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+------------------+--------------------+--------------------+------------------+------------------+\n",
      "|              window|symbol|          avg_volume|    avg_volatility|avg_market_sentiment|avg_trading_activity|         avg_price|             label|\n",
      "+--------------------+------+--------------------+------------------+--------------------+--------------------+------------------+------------------+\n",
      "|{2025-01-11 11:40...|    BP|    9898383.81300813|0.9367886178861792|  0.8148699186991871|   44.50577235772358|30.693414634146336|30.693414634146386|\n",
      "|{2025-01-12 15:40...|    BP|1.2067134131034482E7|0.3864413793103446|-0.04140689655172421|   72.61689655172414|31.040965517241364|31.040965517241503|\n",
      "|{2025-01-13 07:40...|    BP|1.1580768349693252E7|0.6805644171779137|  0.6219141104294478|    46.7493865030675|31.665521472392648|31.665521472392637|\n",
      "|{2025-01-13 10:10...|    BP|1.0686847347457627E7|0.9158389830508478| -0.5117542372881354|   38.05389830508474|31.518898305084758|31.518898305084797|\n",
      "|{2025-01-14 08:30...|    BP|1.3765707589041095E7| 1.094527397260274| -0.8612465753424655|   77.70945205479453|31.453835616438358|  31.4538356164384|\n",
      "+--------------------+------+--------------------+------------------+--------------------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] WARNING: Found 32 null values in label\n",
      "[BP] Schema after feature assembly:\n",
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- symbol: string (nullable = false)\n",
      " |-- avg_volume: double (nullable = true)\n",
      " |-- avg_volatility: double (nullable = true)\n",
      " |-- avg_market_sentiment: double (nullable = true)\n",
      " |-- avg_trading_activity: double (nullable = true)\n",
      " |-- avg_price: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "[BP] Sample data after feature assembly:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------+------------------+\n",
      "|features                                                                                           |label             |\n",
      "+---------------------------------------------------------------------------------------------------+------------------+\n",
      "|[9898383.81300813,0.9367886178861792,0.8148699186991871,44.50577235772358,30.693414634146336]      |30.693414634146386|\n",
      "|[1.2067134131034482E7,0.3864413793103446,-0.04140689655172421,72.61689655172414,31.040965517241364]|31.040965517241503|\n",
      "|[1.1580768349693252E7,0.6805644171779137,0.6219141104294478,46.7493865030675,31.665521472392648]   |31.665521472392637|\n",
      "|[1.0686847347457627E7,0.9158389830508478,-0.5117542372881354,38.05389830508474,31.518898305084758] |31.518898305084797|\n",
      "|[1.3765707589041095E7,1.094527397260274,-0.8612465753424655,77.70945205479453,31.453835616438358]  |31.4538356164384  |\n",
      "+---------------------------------------------------------------------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] WARNING: Found 32 rows with null/NaN features or labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] Cleaned data count: 1302\n",
      "[BP] Training and evaluating models...\n",
      "[BP] Training LinearRegression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] LinearRegression metrics:\n",
      "    RMSE: 0.2115\n",
      "    MAE: 0.1700\n",
      "    R2: 0.8454\n",
      "[BP] Training RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/local/spark/jars/spark-core_2.12-3.5.4.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] RandomForest metrics:\n",
      "    RMSE: 0.9118\n",
      "    MAE: 0.0609\n",
      "    R2: 0.9118\n",
      "[BP] Training GradientBoosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] GradientBoosting metrics:\n",
      "    RMSE: 0.9914\n",
      "    MAE: 0.0293\n",
      "    R2: 0.9914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] Saved historical model to ./10m/historical_models/BP/model_20250117_184031\n",
      "[BP] Best performing model: LinearRegression\n",
      "[BP] Selected model type: LinearRegression\n",
      "[BP] Model evaluation metrics:\n",
      "\n",
      "LinearRegression:\n",
      "    rmse: 0.2115\n",
      "    mae: 0.1700\n",
      "    r2: 0.8454\n",
      "\n",
      "RandomForest:\n",
      "    rmse: 0.9118\n",
      "    mae: 0.0609\n",
      "    r2: 0.9118\n",
      "\n",
      "GradientBoosting:\n",
      "    rmse: 0.9914\n",
      "    mae: 0.0293\n",
      "    r2: 0.9914\n"
     ]
    }
   ],
   "source": [
    "historical_model = train_on_historical_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n"
     ]
    }
   ],
   "source": [
    "print(SELECTED_MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_incoming(batch_df: DataFrame, batch_id: int):\n",
    "    global historical_model, lr_model\n",
    "    \n",
    "    if batch_df.rdd.isEmpty():\n",
    "        print(f\"[{TARGET_SYMBOL}] Predictor batch {batch_id} empty\")\n",
    "        return\n",
    "\n",
    "    if historical_model is None:\n",
    "        historical_model = load_historical_model()\n",
    "    if lr_model is None:\n",
    "        lr_model = load_model()\n",
    "\n",
    "    if lr_model is None or historical_model is None:\n",
    "        print(f\"[{TARGET_SYMBOL}] One or both models not available\")\n",
    "        return\n",
    "\n",
    "    # Get predictions from both models\n",
    "    streaming_predictions = lr_model.transform(batch_df)\n",
    "    historical_predictions = historical_model.transform(batch_df)\n",
    "\n",
    "    # Combine predictions\n",
    "    predictions = streaming_predictions.select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\"),\n",
    "        col(\"event_time\"),\n",
    "        col(\"features\"),\n",
    "        col(\"label\"),\n",
    "        col(\"prediction\").alias(\"prediction\"),\n",
    "        historical_predictions.prediction.alias(\"prediction_historical\")\n",
    "    )\n",
    "\n",
    "    @udf(StringType())\n",
    "    def features_to_json(features):\n",
    "        return json.dumps({\n",
    "            f: float(value) for f, value in zip(FEATURE_COLUMNS, features)\n",
    "        })\n",
    "\n",
    "    predictions_to_save = predictions.withColumn(\n",
    "        \"input_data\", \n",
    "        features_to_json(col(\"features\"))\n",
    "    ).withColumn(\n",
    "        \"label\",\n",
    "        F.lit(None).cast(DoubleType())\n",
    "    )\n",
    "\n",
    "    predictions_to_save.select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"timestamp\"),\n",
    "        col(\"event_time\"),\n",
    "        col(\"input_data\"),\n",
    "        col(\"prediction\"),\n",
    "        col(\"prediction_historical\"),\n",
    "        col(\"label\")\n",
    "    ).write \\\n",
    "     .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "     .mode(\"append\") \\\n",
    "     .options(table=CASSANDRA_TABLE, keyspace=CASSANDRA_KEYSPACE) \\\n",
    "     .save()\n",
    "\n",
    "    print(f\"[{TARGET_SYMBOL}] Completed predictions for batch {batch_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"symbol\", StringType()) \\\n",
    "    .add(\"timestamp\", LongType()) \\\n",
    "    .add(\"source\", StringType()) \\\n",
    "    .add(\"data_type\", StringType()) \\\n",
    "    .add(\"bid\", DoubleType()) \\\n",
    "    .add(\"ask\", DoubleType()) \\\n",
    "    .add(\"price\", DoubleType()) \\\n",
    "    .add(\"volume\", DoubleType()) \\\n",
    "    .add(\"spread_raw\", DoubleType()) \\\n",
    "    .add(\"spread_table\", DoubleType()) \\\n",
    "    .add(\"volatility\", DoubleType()) \\\n",
    "    .add(\"market_sentiment\", DoubleType()) \\\n",
    "    .add(\"trading_activity\", DoubleType())\n",
    "\n",
    "df_kafka = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 180000) \\\n",
    "    .load()\n",
    "\n",
    "parsed_df = df_kafka.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "filtered_df = parsed_df.filter(col(\"symbol\") == TARGET_SYMBOL)\n",
    "\n",
    "LABEL_COLUMN = \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** BEGIN MODIFICATION ***\n",
    "# Correct the 'price' for Ethereum if it's set to -1 by computing the average of 'bid' and 'ask'\n",
    "if TARGET_SYMBOL == \"ETHEREUM\":\n",
    "    # Create a corrected 'price' column\n",
    "    corrected_price = when(col(\"price\") == -1, (col(\"bid\") + col(\"ask\")) / 2).otherwise(col(\"price\"))\n",
    "    \n",
    "    # Apply the corrected 'price' to both 'price' feature and 'label'\n",
    "    processed_df = filtered_df.withColumn(\n",
    "        \"price_corrected\",\n",
    "        corrected_price.cast(\"double\")\n",
    "    ).select(\n",
    "        col(\"symbol\").alias(\"symbol\"),\n",
    "        *[col(feature).cast(\"double\").alias(f\"{feature}\") for feature in FEATURE_COLUMNS if feature != \"price\"],\n",
    "        col(\"price_corrected\").alias(\"price\"),  # Updated 'price' for features\n",
    "        col(\"price_corrected\").alias(\"label\"),  # Updated 'label'\n",
    "        col(\"timestamp\").cast(\"long\").alias(\"timestamp\")  # Include timestamp for Cassandra or other sinks\n",
    "    )\n",
    "else:\n",
    "    # For other symbols, no correction needed\n",
    "    processed_df = filtered_df.select(\n",
    "        col(\"symbol\").alias(\"symbol\"),\n",
    "        *[col(feature).cast(\"double\").alias(f\"{feature}\") for feature in FEATURE_COLUMNS],\n",
    "        col(LABEL_COLUMN).cast(\"double\").alias(\"label\"),  # Use the original 'price' as label\n",
    "        col(\"timestamp\").cast(\"long\").alias(\"timestamp\")  # Include timestamp for Cassandra or other sinks\n",
    "    )\n",
    "# *** END MODIFICATION ***\n",
    "\n",
    "processed_df = processed_df.withColumn(\n",
    "    \"event_time\",\n",
    "    (col(\"timestamp\") / 1000).cast(TimestampType())\n",
    ")\n",
    "\n",
    "# Modified to use 10-minute windows\n",
    "windowed_df = processed_df \\\n",
    "    .withWatermark(\"event_time\", \"20 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"10 minutes\"),\n",
    "        col(\"symbol\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        *[\n",
    "            F.avg(col(feature)).alias(f\"avg_{feature}\") \n",
    "            for feature in FEATURE_COLUMNS\n",
    "        ],\n",
    "        F.avg(col(\"label\")).alias(\"label\")  # Use 10-minute average as label\n",
    "    )\n",
    "\n",
    "aggregated_feature_columns = [f\"avg_{feature}\" for feature in FEATURE_COLUMNS]\n",
    "\n",
    "assembler_agg = VectorAssembler(\n",
    "    inputCols=aggregated_feature_columns,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "windowed_features_df = assembler_agg.transform(windowed_df).select(\n",
    "    \"symbol\",\n",
    "    \"features\",\n",
    "    \"label\",\n",
    "    col(\"window.start\").alias(\"window_start\"),\n",
    "    col(\"window.end\").alias(\"window_end\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] Started aggregator query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1723:>                                                      (0 + 2) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] One or both models not available\n",
      "[BP] One or both models not available\n",
      "[BP] One or both models not available\n",
      "[BP] Updated model with batch 0\n",
      "[BP] Coefficients: [0.0,0.0,0.0,0.0,0.6157312044966592]\n",
      "[BP] Intercept: 12.176984886827118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_8707/1532492292.py\", line 22, in predict_incoming\n",
      "    predictions = streaming_predictions.select(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/pyspark/sql/dataframe.py\", line 3229, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [MISSING_ATTRIBUTES.RESOLVED_ATTRIBUTE_APPEAR_IN_OPERATION] Resolved attribute(s) \"prediction\" missing from \"symbol\", \"features\", \"label\", \"timestamp\", \"event_time\", \"prediction\" in operator !Project [symbol#2214, timestamp#2221L, event_time#2230, features#2306, label#2220, prediction#2573 AS prediction#2589, prediction#2582 AS prediction_historical#2590]. Attribute(s) with the same name appear in the operation: \"prediction\".\n",
      "Please check if the right attribute(s) are used.;\n",
      "!Project [symbol#2214, timestamp#2221L, event_time#2230, features#2306, label#2220, prediction#2573 AS prediction#2589, prediction#2582 AS prediction_historical#2590]\n",
      "+- Project [symbol#2214, features#2306, label#2220, timestamp#2221L, event_time#2230, UDF(features#2306) AS prediction#2573]\n",
      "   +- LogicalRDD [symbol#2214, features#2306, label#2220, timestamp#2221L, event_time#2230], false\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] Updated model with batch 1\n",
      "[BP] Coefficients: [0.0,0.0,0.0,0.0,0.0]\n",
      "[BP] Intercept: 31.68053672316384\n",
      "[BP] Updated model with batch 2\n",
      "[BP] Coefficients: [0.0,0.0,0.0,0.0,0.0]\n",
      "[BP] Intercept: 31.669157608695663\n",
      "[BP] Updated model with batch 3\n",
      "[BP] Coefficients: [0.0,0.0,0.0,0.0,0.0]\n",
      "[BP] Intercept: 31.6842257217848\n"
     ]
    }
   ],
   "source": [
    "# Start aggregator stream with 10-minute trigger\n",
    "query_agg = windowed_features_df.writeStream \\\n",
    "    .foreachBatch(update_model) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH_AGG) \\\n",
    "    .trigger(processingTime='10 minutes') \\\n",
    "    .start()\n",
    "\n",
    "print(f\"[{TARGET_SYMBOL}] Started aggregator query\")\n",
    "\n",
    "# Predictor stream setup\n",
    "assembler_pred = VectorAssembler(\n",
    "    inputCols=FEATURE_COLUMNS,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Process each record immediately for prediction\n",
    "processed_df_for_pred = assembler_pred.transform(\n",
    "    processed_df.select(\"symbol\", *FEATURE_COLUMNS, \"label\", \"timestamp\", \"event_time\")\n",
    ").select(\n",
    "    \"symbol\",\n",
    "    \"features\",\n",
    "    \"label\",\n",
    "    \"timestamp\",\n",
    "    \"event_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1723:==>                                                    (1 + 2) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BP] Started predictor query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1724:>               (0 + 1) / 1][Stage 1726:>             (1 + 1) / 200]\r"
     ]
    }
   ],
   "source": [
    "# Start predictor stream with minimal trigger interval\n",
    "query_pred = processed_df_for_pred.writeStream \\\n",
    "    .foreachBatch(predict_incoming) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH_PRED) \\\n",
    "    .trigger(processingTime='1 second') \\\n",
    "    .start()\n",
    "\n",
    "print(f\"[{TARGET_SYMBOL}] Started predictor query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command. 1][Stage 1729:> (0 + 0) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[Stage 1726:(174 + 2) / 200][Stage 1728:> (0 + 0) / 1][Stage 1729:> (0 + 0) / 1]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start label updater stream\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query_labels \u001b[38;5;241m=\u001b[39m processed_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(update_labels) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10 minutes\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitAnyTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pyspark/sql/streaming/query.py:596\u001b[0m, in \u001b[0;36mStreamingQueryManager.awaitAnyTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsqm\u001b[38;5;241m.\u001b[39mawaitAnyTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsqm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitAnyTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/17 18:59:29 ERROR MicroBatchExecution: Query [id = 8ea2d66a-d522-4d26-8e38-76d0ead7c5c4, runId = ccbd6907-210e-42e7-a903-bcdcc053d720] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_8707/1532492292.py\", line 22, in predict_incoming\n",
      "    predictions = streaming_predictions.select(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/pyspark/sql/dataframe.py\", line 3229, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [MISSING_ATTRIBUTES.RESOLVED_ATTRIBUTE_APPEAR_IN_OPERATION] Resolved attribute(s) \"prediction\" missing from \"symbol\", \"features\", \"label\", \"timestamp\", \"event_time\", \"prediction\" in operator !Project [symbol#2214, timestamp#2221L, event_time#2230, features#2306, label#2220, prediction#2573 AS prediction#2589, prediction#2582 AS prediction_historical#2590]. Attribute(s) with the same name appear in the operation: \"prediction\".\n",
      "Please check if the right attribute(s) are used.;\n",
      "!Project [symbol#2214, timestamp#2221L, event_time#2230, features#2306, label#2220, prediction#2573 AS prediction#2589, prediction#2582 AS prediction_historical#2590]\n",
      "+- Project [symbol#2214, features#2306, label#2220, timestamp#2221L, event_time#2230, UDF(features#2306) AS prediction#2573]\n",
      "   +- LogicalRDD [symbol#2214, features#2306, label#2220, timestamp#2221L, event_time#2230], false\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy47.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "25/01/17 19:30:00 ERROR MicroBatchExecution: Query [id = ce7fc661-336b-4f09-9b47-9f1a0e84bc5e, runId = d61d3a68-2aaf-4346-85f8-e74c76da78fd] terminated with error\n",
      "org.apache.spark.SparkException: [CONCURRENT_STREAM_LOG_UPDATE] Concurrent update to the log. Multiple streaming jobs detected for 4.\n",
      "Please make sure only one streaming job runs on a specific checkpoint location at a time.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.concurrentStreamLogUpdate(QueryExecutionErrors.scala:1303)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.markMicroBatchStart(MicroBatchExecution.scala:765)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$13(MicroBatchExecution.scala:536)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:535)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "25/01/17 21:33:03 ERROR MicroBatchExecution: Query [id = 5566314a-0ce9-4c49-a321-2946052d4549, runId = c2d9f7c8-e5dc-49ce-b304-2f169957d9a3] terminated with error\n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeTopics\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2005)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeTopics\n"
     ]
    }
   ],
   "source": [
    "# Start label updater stream\n",
    "query_labels = processed_df.writeStream \\\n",
    "    .foreachBatch(update_labels) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime='10 minutes') \\\n",
    "    .start()\n",
    "\n",
    "spark.streams.awaitAnyTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
